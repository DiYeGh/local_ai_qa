项目开发方案：本地知识库 AI 问答系统（Windows 环境）该系统的目的是开发一个本地知识库的 AI 问答系统，支持通过文件上传、文档解析、文本嵌入生成与检索，实现对文档中知识的高效提问与回答。系统的主要组件包括文档上传与解析、嵌入生成与存储、混合检索（结合向量与 BM25）、AI 答复生成、以及前后端交互。系统的开发采用 Windows 环境，后端使用 FastAPI，向量数据库使用 Milvus，AI 模型部署使用 Ollama。
一、文档上传与解析
1、使用 FastAPI 提供文档上传接口，支持用户上传 Word、TXT文档格式。需要文件类型验证与文件大小限制30M，确保用户上传符合要求的文件。
2、文档内容提取与分块，对不同文档格式使用合适的解析库进行处理：Word 文档使用 python-docx，TXT 文件直接读取文本内容。
3、将提取的文档内容划分为多个较小的文本块，推荐每块 500-1000 字符，或按段落/句子进行划分，确保语义连贯。
4、文本嵌入生成使用部署在 Ollama 容器中的 bge-m3 模型，生成每个文本块的嵌入（embedding）。使用异步或多线程方式进行并行化处理
5、使用 Milvus 数据库存储内容包括文档 ID、文档名、块的索引信息及嵌入向量，创建合适的索引结构，以便后续高效检索。

二、混合检索与生成（RAG）
1、FastAPI 提供提问接口，接收用户的问题并返回答案。
2、利用接口传递的问题通过 Milvus 向量数据库进行向量检索，获取与问题相关的文档块，结合 BM25 算法，对问题中的关键词进行全文检索，获取相关文档块。
3、使用加权策略结合两种检索方式的结果，根据向量相似度和 BM25 得分对结果进行排序。
4、将从向量检索和 BM25 检索中获得的候选文档块输入至 Ollama 的 bge-reranker-v2-m3 重排序模型，优化文档块的排序结果选择3个块。
5、将重排序后前3的文档块拼接成上下文，并生成合适的 prompt，输入到 Ollama 部署的 AI 模型（例如 deepseek-r1:7b）进行问答。
6、将 AI 模型返回的答案作为问题的响应结果，返回给用户。

三、前后端交互
前端与后端交互
前端通过 RESTful API 与后端进行交互，提问接口与文档上传接口均通过 HTTP 请求实现。
支持文件上传进度条显示，问题提交进度反馈等交互设计，确保用户体验。
前端技术
前端使用现代 JavaScript 框架（如 React 或 Vue）实现用户界面，支持文件上传、问题提问、展示回答等功能。
前端与后端通过异步请求（如 axios）进行交互，并处理返回的 JSON 格式数据。
缓存与优化

对常见问题的答案使用缓存机制（如 Redis），减少重复计算和加快响应速度。
可考虑为用户提供会话缓存，存储用户查询的上下文，提升多轮问答的流畅性。